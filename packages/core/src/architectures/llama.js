// NOTE: This file has been auto-generated. Do not edit directly.

export default {
  model_type: "llama",
  models: [
    {
      model_id: "HuggingFaceTB/SmolLM2-1.7B-Instruct",
      dtype: "q4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Constant",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulNBits",
        "Mul",
        "ReduceSum",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "HuggingFaceTB/SmolLM2-1.7B-Instruct",
      dtype: "bnb4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Constant",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulBnb4",
        "Mul",
        "ReduceSum",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "HuggingFaceTB/SmolLM2-1.7B-Instruct",
      dtype: "quantized",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Constant",
        "DequantizeLinear",
        "DynamicQuantizeLinear",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulInteger",
        "Mul",
        "ReduceSum",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "HuggingFaceTB/SmolLM2-1.7B-Instruct",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Constant",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "Mul",
        "ReduceSum",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "HuggingFaceTB/SmolLM2-360M-Instruct",
      dtype: "q4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulNBits",
        "Mul",
        "ReduceSum",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "HuggingFaceTB/SmolLM2-360M-Instruct",
      dtype: "bnb4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulBnb4",
        "Mul",
        "ReduceSum",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "HuggingFaceTB/SmolLM2-360M-Instruct",
      dtype: "quantized",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "DequantizeLinear",
        "DynamicQuantizeLinear",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulInteger",
        "Mul",
        "ReduceSum",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "HuggingFaceTB/SmolLM2-360M-Instruct",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "Mul",
        "ReduceSum",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "Xenova/LiteLlama-460M-1T",
      dtype: "quantized",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "DequantizeLinear",
        "Div",
        "DynamicQuantizeLinear",
        "Equal",
        "Expand",
        "Gather",
        "Less",
        "MatMul",
        "MatMulInteger",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "Shape",
        "Sigmoid",
        "Slice",
        "Softmax",
        "Sqrt",
        "Squeeze",
        "Sub",
        "Transpose",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/LiteLlama-460M-1T",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Div",
        "Equal",
        "Expand",
        "Gather",
        "Less",
        "MatMul",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "Shape",
        "Sigmoid",
        "Slice",
        "Softmax",
        "Sqrt",
        "Squeeze",
        "Sub",
        "Transpose",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/TinyLLama-v0",
      dtype: "quantized",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "DequantizeLinear",
        "Div",
        "DynamicQuantizeLinear",
        "Equal",
        "Expand",
        "Gather",
        "Identity",
        "If",
        "Less",
        "MatMul",
        "MatMulInteger",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "Shape",
        "Sigmoid",
        "Slice",
        "Softmax",
        "Sqrt",
        "Squeeze",
        "Sub",
        "Transpose",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/TinyLLama-v0",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Div",
        "Equal",
        "Expand",
        "Gather",
        "Identity",
        "If",
        "Less",
        "MatMul",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "Shape",
        "Sigmoid",
        "Slice",
        "Softmax",
        "Sqrt",
        "Squeeze",
        "Sub",
        "Transpose",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/TinyLlama-1.1B-Chat-v1.0",
      dtype: "quantized",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Cos",
        "DequantizeLinear",
        "Div",
        "DynamicQuantizeLinear",
        "Equal",
        "Expand",
        "Gather",
        "Greater",
        "Less",
        "MatMul",
        "MatMulInteger",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Squeeze",
        "Sub",
        "Transpose",
        "Trilu",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/TinyLlama-1.1B-Chat-v1.0",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Cos",
        "Div",
        "Equal",
        "Expand",
        "Gather",
        "Greater",
        "Less",
        "MatMul",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Squeeze",
        "Sub",
        "Transpose",
        "Trilu",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/llama2.c-stories15M",
      dtype: "q4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "ConstantOfShape",
        "Cos",
        "Div",
        "Equal",
        "Expand",
        "Gather",
        "Greater",
        "MatMul",
        "MatMulNBits",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Transpose",
        "Trilu",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/llama2.c-stories15M",
      dtype: "quantized",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Cos",
        "DequantizeLinear",
        "Div",
        "DynamicQuantizeLinear",
        "Equal",
        "Expand",
        "Gather",
        "Greater",
        "Identity",
        "If",
        "Less",
        "MatMul",
        "MatMulInteger",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Squeeze",
        "Sub",
        "Transpose",
        "Trilu",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/llama2.c-stories15M",
      dtype: "bnb4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "ConstantOfShape",
        "Cos",
        "Div",
        "Equal",
        "Expand",
        "Gather",
        "Greater",
        "MatMul",
        "MatMulBnb4",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Transpose",
        "Trilu",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/llama2.c-stories15M",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Cos",
        "Div",
        "Equal",
        "Expand",
        "Gather",
        "Greater",
        "Identity",
        "If",
        "Less",
        "MatMul",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Squeeze",
        "Sub",
        "Transpose",
        "Trilu",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/llama2.c-stories15M",
      dtype: "uint8",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "ConstantOfShape",
        "Cos",
        "DequantizeLinear",
        "Div",
        "DynamicQuantizeLinear",
        "Equal",
        "Expand",
        "Gather",
        "Greater",
        "MatMul",
        "MatMulInteger",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Transpose",
        "Trilu",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/llama2.c-stories15M",
      dtype: "fp16",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "ConstantOfShape",
        "Cos",
        "Div",
        "Equal",
        "Expand",
        "Gather",
        "Greater",
        "MatMul",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Transpose",
        "Trilu",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "Xenova/tiny-random-LlamaForCausalLM-optimized",
      dtype: "q4f16",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Equal",
        "Expand",
        "Gather",
        "Less",
        "MatMulNBits",
        "Mul",
        "MultiHeadAttention",
        "Range",
        "Reshape",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Slice",
        "Squeeze",
        "Sub",
        "Tile",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "hf-internal-testing/tiny-random-LlamaForCausalLM",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "And",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Cos",
        "Div",
        "Equal",
        "Expand",
        "Gather",
        "Identity",
        "MatMul",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Tile",
        "Transpose",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "onnx-community/Llama-3.2-1B-Instruct-q4f16",
      dtype: "q4f16",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Equal",
        "Expand",
        "Gather",
        "Less",
        "MatMulNBits",
        "Mul",
        "MultiHeadAttention",
        "Range",
        "Reshape",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Slice",
        "Squeeze",
        "Sub",
        "Tile",
        "Transpose",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "onnx-community/Pleias-Pico",
      dtype: "q4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulNBits",
        "Mul",
        "ReduceSum",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "onnx-community/Pleias-Pico",
      dtype: "bnb4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulBnb4",
        "Mul",
        "ReduceSum",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "onnx-community/Pleias-Pico",
      dtype: "quantized",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "DequantizeLinear",
        "DynamicQuantizeLinear",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulInteger",
        "Mul",
        "ReduceSum",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "onnx-community/Pleias-Pico",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "Mul",
        "ReduceSum",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "onnx-community/tiny-random-LlamaForCausalLM-ONNX",
      dtype: "q4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Gather",
        "GroupQueryAttention",
        "MatMulNBits",
        "Mul",
        "ReduceSum",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
      ],
    },
    {
      model_id: "onnx-community/tiny-random-LlamaForCausalLM-ONNX",
      dtype: "q4f16",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "ConstantOfShape",
        "Equal",
        "Expand",
        "Gather",
        "Less",
        "MatMulNBits",
        "Mul",
        "MultiHeadAttention",
        "Range",
        "Reshape",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Slice",
        "Squeeze",
        "Sub",
        "Tile",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "onnx-community/tiny-random-LlamaForCausalLM-ONNX",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "ConstantOfShape",
        "Equal",
        "Expand",
        "Gather",
        "Less",
        "MatMul",
        "Mul",
        "MultiHeadAttention",
        "Range",
        "Reshape",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Slice",
        "Squeeze",
        "Sub",
        "Tile",
        "Unsqueeze",
        "Where",
      ],
    },
    {
      model_id: "onnx-community/turn-detector-GQA-ONNX",
      dtype: "q4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Constant",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulNBits",
        "Mul",
        "ReduceSum",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "onnx-community/turn-detector-GQA-ONNX",
      dtype: "bnb4",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Constant",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulBnb4",
        "Mul",
        "ReduceSum",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "onnx-community/turn-detector-GQA-ONNX",
      dtype: "quantized",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Constant",
        "DequantizeLinear",
        "DynamicQuantizeLinear",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "MatMulInteger",
        "Mul",
        "ReduceSum",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "onnx-community/turn-detector-GQA-ONNX",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Constant",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "Mul",
        "ReduceSum",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
        "Transpose",
      ],
    },
    {
      model_id: "onnx-internal-testing/tiny-random-LlamaForCausalLM-GQA",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Cast",
        "Constant",
        "Gather",
        "GroupQueryAttention",
        "MatMul",
        "Mul",
        "ReduceSum",
        "RotaryEmbedding",
        "Shape",
        "Sigmoid",
        "SimplifiedLayerNormalization",
        "SkipSimplifiedLayerNormalization",
        "Sub",
      ],
    },
    {
      model_id: "onnx-internal-testing/tiny-random-LlamaForCausalLM-ONNX",
      dtype: "fp32",
      architectures: ["LlamaForCausalLM"],
      ops: [
        "Add",
        "Cast",
        "Concat",
        "Constant",
        "ConstantOfShape",
        "Cos",
        "Div",
        "Equal",
        "Expand",
        "Gather",
        "Greater",
        "Identity",
        "MatMul",
        "Mul",
        "Neg",
        "Pow",
        "Range",
        "ReduceMean",
        "Reshape",
        "ScatterND",
        "Shape",
        "Sigmoid",
        "Sin",
        "Slice",
        "Softmax",
        "Sqrt",
        "Transpose",
        "Trilu",
        "Unsqueeze",
        "Where",
      ],
    },
  ],
};
